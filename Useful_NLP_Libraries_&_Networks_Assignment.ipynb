{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Theory"
      ],
      "metadata": {
        "id": "K6K2g8MQ3ToD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Compare and contrast NLTK and spaCy in terms of features, ease of use, and performance.\n",
        "- I) Tokenization:\n",
        "  - i) NLTK: Rule-based tokenizers; customizable but slower.  \n",
        "  - i) spaCy: Highly optimized tokenizer written in Cython (very fast).  \n",
        "- II) POS Tagging:\n",
        "  - i) NLTK: Several taggers (Perceptron, CRF, etc.); customizable.\n",
        "  - i) spaCy: Pre-trained statistical models for multiple languages.\n",
        "- III) Named Entity Recognition:\n",
        "  - i) NLTK: Basic; models are older and less accurate.\n",
        "  - i) spaCy: State-of-the-art pre-trained models with good accuracy.\n",
        "- IV) Dependency Parsing:\n",
        "  - i) NLTK: Limited (via external tools like Stanford Parser).\n",
        "  - i) spaCy: Built-in dependency parser, efficient and accurate.\n",
        "- V) Lemmatization:\n",
        "  - i) NLTK: WordNet-based; good for English only.\n",
        "  - i) spaCy: Built-in lemmatizer trained on large corpora.\n",
        "\n",
        "2. What is TextBlob and how does it simplify common NLP tasks like sentiment analysis and translation?\n",
        "- TextBlob is a high-level NLP (Natural Language Processing) library for Python that builds on top of NLTK and Pattern to make text processing simpler and more intuitive.\n",
        "It’s designed to help developers and beginners perform common NLP tasks — such as sentiment analysis, part-of-speech tagging, noun phrase extraction, translation, and language detection — using just a few lines of code.\n",
        "\n",
        "3. Explain the role of Standford NLP in academic and industry NLP Projects.\n",
        "- Stanford NLP (Stanford CoreNLP) is one of the most influential and widely used natural language processing (NLP) frameworks developed by the Stanford Natural Language Processing Group at Stanford University. It plays a major role in both academic research and industry applications due to its rich linguistic tools, high accuracy, and flexibility.\n",
        "- I) Role in Academic Research:\n",
        "  - i) Benchmark models: Many research papers cite or use Stanford POS Tagger, NER, or Parser as baselines.\n",
        "  - ii) Corpora & datasets: It provides trained models on standard linguistic datasets (Penn Treebank, OntoNotes, etc.).\n",
        "  - iii) Reproducibility: Enables researchers to reproduce linguistic analyses consistently.\n",
        "- II) Role in Industry Applications:\n",
        "  - i) Information Extraction:- Extracting entities and relations from news legal, and financial documents.\n",
        "  - ii) Chatbots & Virtual Assistants:- Understanding user intent and parsing language structure.\n",
        "  - iii) Search & Recommendation Systems:- Improving semantic search using syntactic and entity information.\n",
        "\n",
        "4. Describe the architecture and functioning of a Recurrent Natural Networ(RNN).\n",
        "- An RNN's architecture is a series of interconnected units that process sequential data, featuring a \"feedback loop\" where the output from a previous step is fed as an input to the current step.\n",
        "- Architecture:\n",
        "  - i) Recurrent connections: The core feature is a loop that sends the output of a neuron (or layer) back into itself.\n",
        "  - ii) Hidden state: This is the memory of the network. At each time step, a new hidden state is computed based on the current input and the previous hidden state.\n",
        "  - iii) Layers: Similar to other neural networks, RNNs have input, output, and hidden layers. The hidden layer is where the \"recurrent\" processing happens.\n",
        "\n",
        "5. What is the key difference between LSTM and GRU networks in NLP applications?\n",
        "- I) Number of Gates\n",
        "  - i) LSTM:- 3 gates: Input Gate, Forget Gate, Output Gate.\n",
        "  - i) GRU:- 2 gates: Update Gate, Reset Gate.\n",
        "- II) Memory Components\n",
        "  - i) LSTM:- Has two states:\n",
        "     - 1. Cell state (Cₜ) — long-term memory\n",
        "     - 2. Hidden state (hₜ) — short-term memory\n",
        "  - i) GRU:- Has one state (hₜ) that combines both long- and short-term memory.\n",
        "- III) Complexity\n",
        "  - i) LSTM:- More parameters and more computation (heavier model)\n",
        "  - i) GRU:- Fewer parameters and simpler (lighter model)\n",
        "- IV) Training Speed\n",
        "  - i) LSTM:- Slower due to complex gating\n",
        "  - i) GRU:- Faster to train\n",
        "- V) Architecture Depth\n",
        "  - i) LSTM:- Deeper and more expressive.\n",
        "  - i) GRU:- Compact and faster to converge.\n"
      ],
      "metadata": {
        "id": "e2vEF0z36TBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Write a Python program using TextBlob to perform sentiment analysis on the following paragraph of text:\n",
        "# “I had a great experience using the new mobile banking app. The interface is intuitive, and customer support was quick to resolve my issue. However, the app did crash once during a transaction, which was frustrating\"\n",
        "# Your program should print out the polarity and subjectivity scores.\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Input paragraph\n",
        "text = \"\"\"I had a great experience using the new mobile banking app.\n",
        "The interface is intuitive, and customer support was quick to resolve my issue.\n",
        "However, the app did crash once during a transaction, which was frustrating.\"\"\"\n",
        "\n",
        "# Create a TextBlob object\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Get sentiment analysis\n",
        "sentiment = blob.sentiment\n",
        "\n",
        "# Print the results\n",
        "print(\"Sentiment Analysis Results:\")\n",
        "print(f\"Polarity: {sentiment.polarity}\")\n",
        "print(f\"Subjectivity: {sentiment.subjectivity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyY6NeyNfAMV",
        "outputId": "e564b8c8-45c4-41b0-dde1-fecdf7f17f26"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Analysis Results:\n",
            "Polarity: 0.21742424242424244\n",
            "Subjectivity: 0.6511363636363636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Given the sample paragraph below, perform string tokenization and frequency distribution using Python and NLTK:\n",
        "# “Natural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence. It enables machines to understand, interpret, and generate human language.\n",
        "# Applications of NLP include chatbots, sentiment analysis, and machine translation. As technology advances, the role of NLP in modern solutions is becoming increasingly critical.”\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Sample paragraph\n",
        "text = \"\"\"Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.\"\"\"\n",
        "\n",
        "# Download required NLTK data (run once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Display tokens\n",
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "print(\"\\nTotal Tokens:\", len(tokens))\n",
        "\n",
        "# Create frequency distribution\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "# Display the most common words\n",
        "print(\"\\nFrequency Distribution (Top 10):\")\n",
        "for word, freq in freq_dist.most_common(10):\n",
        "    print(f\"{word}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYgSVbgdfuav",
        "outputId": "cc1291cc-c002-4933-a4ed-604046a8f571"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.', 'It', 'enables', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'Applications', 'of', 'NLP', 'include', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'machine', 'translation', '.', 'As', 'technology', 'advances', ',', 'the', 'role', 'of', 'NLP', 'in', 'modern', 'solutions', 'is', 'becoming', 'increasingly', 'critical', '.']\n",
            "\n",
            "Total Tokens: 63\n",
            "\n",
            "Frequency Distribution (Top 10):\n",
            ",: 7\n",
            ".: 4\n",
            "NLP: 3\n",
            "and: 3\n",
            "is: 2\n",
            "of: 2\n",
            "Natural: 1\n",
            "Language: 1\n",
            "Processing: 1\n",
            "(: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Implement a basic LSTM model in Keras for a text classification task using the following dummy dataset. Your model should classify sentences as either positive (1) or negative (0).\n",
        "# Dataset\n",
        "# texts = [\n",
        "# “I love this project”, #Positive\n",
        "# “This is an amazing experience”, #Positive\n",
        "# “I hate waiting in line”, #Negative\n",
        "# “This is the worst service”, #Negative\n",
        "# “Absolutely fantastic!” #Positive]\n",
        "# labels = [1, 1, 0, 0, 1]\n",
        "# Preprocess the text, tokenize it, pad sequences, and build an LSTM model to train on this data. You may use Keras with TensorFlow backend.\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "texts = [\n",
        "    \"I love this project\",          # Positive\n",
        "    \"This is an amazing experience\",# Positive\n",
        "    \"I hate waiting in line\",       # Negative\n",
        "    \"This is the worst service\",    # Negative\n",
        "    \"Absolutely fantastic!\"         # Positive\n",
        "]\n",
        "labels = [1, 1, 0, 0, 1]  # 1 = Positive, 0 = Negative\n",
        "labels = np.array(labels)\n",
        "tokenizer = Tokenizer(num_words=100)  # Only consider top 100 words\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Convert texts to sequences of integers\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Check sequences\n",
        "print(\"Sequences:\", sequences)\n",
        "max_len = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "print(\"Padded Sequences:\\n\", padded_sequences)\n",
        "vocab_size = 100  # same as tokenizer num_words\n",
        "embedding_dim = 16\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))\n",
        "model.add(LSTM(32))  # 32 LSTM units\n",
        "model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary\n",
        "model.summary()\n",
        "model.fit(padded_sequences, labels, epochs=20, batch_size=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m6kAMNarg8zf",
        "outputId": "c081f5b6-8f7b-41a8-aa09-169fc36d7cbb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequences: [[2, 4, 1, 5], [1, 3, 6, 7, 8], [2, 9, 10, 11, 12], [1, 3, 13, 14, 15], [16, 17]]\n",
            "Padded Sequences:\n",
            " [[ 2  4  1  5  0]\n",
            " [ 1  3  6  7  8]\n",
            " [ 2  9 10 11 12]\n",
            " [ 1  3 13 14 15]\n",
            " [16 17  0  0  0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6444 - loss: 0.6912\n",
            "Epoch 2/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8917 - loss: 0.6843 \n",
            "Epoch 3/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.6855 \n",
            "Epoch 4/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.6836 \n",
            "Epoch 5/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.6784 \n",
            "Epoch 6/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.6588 \n",
            "Epoch 7/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.6546 \n",
            "Epoch 8/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.6474 \n",
            "Epoch 9/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.6437 \n",
            "Epoch 10/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.6010 \n",
            "Epoch 11/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.5924 \n",
            "Epoch 12/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.5740 \n",
            "Epoch 13/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.5254 \n",
            "Epoch 14/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.5359 \n",
            "Epoch 15/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.3477 \n",
            "Epoch 16/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.3170 \n",
            "Epoch 17/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.3576 \n",
            "Epoch 18/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.2352 \n",
            "Epoch 19/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.2286 \n",
            "Epoch 20/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.1289 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c4790e712e0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Using spaCy, build a simple NLP pipeline that includes tokenization, lemmatization, and entity recognition. Use the following paragraph as your dataset:\n",
        "# “Homi Jehangir Bhaba was an Indian nuclear physicist who played a key role in the development of India’s atomic energy program. He was the founding director of the Tata Institute of Fundamental Research (TIFR)\n",
        "# and was instrumental in establishing the Atomic Energy Commission of India.”\n",
        "# Write a Python program that processes this text using spaCy, then prints tokens, their lemmas, and any named entities found.\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load the English model (download if not already)\n",
        "# !python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample paragraph\n",
        "text = \"\"\"Homi Jehangir Bhaba was an Indian nuclear physicist who played a key role in the\n",
        "development of India’s atomic energy program. He was the founding director of the Tata\n",
        "Institute of Fundamental Research (TIFR) and was instrumental in establishing the\n",
        "Atomic Energy Commission of India.\"\"\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# --- Tokenization and Lemmatization ---\n",
        "print(\"Tokens and Lemmas:\")\n",
        "for token in doc:\n",
        "    print(f\"Token: {token.text}\\t Lemma: {token.lemma_}\")\n",
        "\n",
        "# --- Named Entity Recognition ---\n",
        "print(\"\\nNamed Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entity: {ent.text}\\t Label: {ent.label_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qs-QyYboVcX",
        "outputId": "691217fd-1541-413b-f1a5-223596cd45b1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens and Lemmas:\n",
            "Token: Homi\t Lemma: Homi\n",
            "Token: Jehangir\t Lemma: Jehangir\n",
            "Token: Bhaba\t Lemma: Bhaba\n",
            "Token: was\t Lemma: be\n",
            "Token: an\t Lemma: an\n",
            "Token: Indian\t Lemma: indian\n",
            "Token: nuclear\t Lemma: nuclear\n",
            "Token: physicist\t Lemma: physicist\n",
            "Token: who\t Lemma: who\n",
            "Token: played\t Lemma: play\n",
            "Token: a\t Lemma: a\n",
            "Token: key\t Lemma: key\n",
            "Token: role\t Lemma: role\n",
            "Token: in\t Lemma: in\n",
            "Token: the\t Lemma: the\n",
            "Token: \n",
            "\t Lemma: \n",
            "\n",
            "Token: development\t Lemma: development\n",
            "Token: of\t Lemma: of\n",
            "Token: India\t Lemma: India\n",
            "Token: ’s\t Lemma: ’s\n",
            "Token: atomic\t Lemma: atomic\n",
            "Token: energy\t Lemma: energy\n",
            "Token: program\t Lemma: program\n",
            "Token: .\t Lemma: .\n",
            "Token: He\t Lemma: he\n",
            "Token: was\t Lemma: be\n",
            "Token: the\t Lemma: the\n",
            "Token: founding\t Lemma: found\n",
            "Token: director\t Lemma: director\n",
            "Token: of\t Lemma: of\n",
            "Token: the\t Lemma: the\n",
            "Token: Tata\t Lemma: Tata\n",
            "Token: \n",
            "\t Lemma: \n",
            "\n",
            "Token: Institute\t Lemma: Institute\n",
            "Token: of\t Lemma: of\n",
            "Token: Fundamental\t Lemma: Fundamental\n",
            "Token: Research\t Lemma: Research\n",
            "Token: (\t Lemma: (\n",
            "Token: TIFR\t Lemma: TIFR\n",
            "Token: )\t Lemma: )\n",
            "Token: and\t Lemma: and\n",
            "Token: was\t Lemma: be\n",
            "Token: instrumental\t Lemma: instrumental\n",
            "Token: in\t Lemma: in\n",
            "Token: establishing\t Lemma: establish\n",
            "Token: the\t Lemma: the\n",
            "Token: \n",
            "\t Lemma: \n",
            "\n",
            "Token: Atomic\t Lemma: Atomic\n",
            "Token: Energy\t Lemma: Energy\n",
            "Token: Commission\t Lemma: Commission\n",
            "Token: of\t Lemma: of\n",
            "Token: India\t Lemma: India\n",
            "Token: .\t Lemma: .\n",
            "\n",
            "Named Entities:\n",
            "Entity: Homi Jehangir Bhaba\t Label: FAC\n",
            "Entity: Indian\t Label: NORP\n",
            "Entity: India\t Label: GPE\n",
            "Entity: the Tata\n",
            "Institute of Fundamental Research\t Label: ORG\n",
            "Entity: Atomic Energy Commission of India\t Label: ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working on a chatbot for a mental health platform. Explain how you would leverage LSTM or GRU networks along with libraries like spaCy or Stanford NLP to understand and respond to user input effectively. Detail your architecture, data preprocessing pipeline, and any ethical considerations.\n",
        "- I) Data Preprocessing Pipeline\n",
        "Before feeding data into an LSTM or GRU, we must clean, normalize, and encode text. NLP libraries help with this:\n",
        "  - i) Text Cleaning\n",
        "\n",
        "    - Lowercasing, removing unnecessary punctuation.\n",
        "\n",
        "    - Removing sensitive or personally identifiable information (PII) for privacy.\n",
        "\n",
        "  - ii) Tokenization & Lemmatization\n",
        "\n",
        "    - Use spaCy or Stanford NLP to:\n",
        "\n",
        "      - Split text into tokens (words)\n",
        "\n",
        "      - Extract lemmas (base word forms)\n",
        "\n",
        "     - Example: “I’ve been feeling anxious” → [\"I\", \"have\", \"be\", \"feel\", \"anxious\"]\n",
        "\n",
        "  - iii) Named Entity Recognition (NER)\n",
        "\n",
        "    - Identify names, dates, locations, or organizations to anonymize sensitive information.\n",
        "\n",
        "    - Example: Replace “John” with <PERSON>.\n",
        "\n",
        "  - iv) Word Embeddings\n",
        "\n",
        "    - Convert tokens into numerical vectors:\n",
        "\n",
        "    - Pre-trained embeddings like GloVe or Word2Vec Or trainable embeddings in Keras embedding layer"
      ],
      "metadata": {
        "id": "JXmvYf3Lpdh9"
      }
    }
  ]
}